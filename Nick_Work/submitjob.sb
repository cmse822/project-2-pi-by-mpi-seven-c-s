#!/bin/bash
########## Define Resources Needed with SBATCH Lines ##########
#SBATCH --array=0-2
#SBATCH --time=00:10:00             # limit of wall clock time - how long the job will run (same as -t)
#SBATCH --nodes=1                  # number of tasks - how many tasks (nodes) that you require (same as -n)
#SBATCH --cpus-per-task=64           # number of CPUs (or cores) per task (same as -c)
#SBATCH --mem=2G                    # memory required per node - amount of memory (in bytes)
#SBATCH --job-name calc_pi     # you can give your job a name for easier identification (same as -J)
#SBATCH --output=output/pi_job_%A_%a.out
#SBATCH --error=output/pi_job_%A_%a.err
 
 
########## Command Lines to Run ##########

module purge
module load gcc/7.3.0-2.30 openmpi hdf5 python git
  
cd ~/Documents/project-2-pi-by-mpi-seven-c-s/Nick_Work            ### change to the directory where your code is located

num_darts=(1e3 1e6 1e9)
for cpus in 1 2 4 8 16 32 64
do 

    output = mpiexec -n $cpus ./pi ${num_darts[$SLURM_ARRAY_TASK_ID]}             ### call your executable
    echo -e "${output},${cpus},${num_darts[$SLURM_ARRAY_TASK_ID]} "

done
scontrol show job $SLURM_JOB_ID     ### write job information to output file
